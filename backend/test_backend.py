#!/usr/bin/env python3
"""
Simple test script for the Multi-Source Analysis Agent backend

This script tests basic functionality without requiring external files.
Run with: python test_backend.py
"""

import asyncio
import json
from agent import MultiSourceAnalysisAgent
from tools.rag_tool import RAGTool
from tools.tavily_tool import TavilyTool
from tools.data_commons_tool import DataCommonsTool

async def test_tools():
    """Test individual tools"""
    print("ğŸ§ª Testing individual tools...")
    
    # Test Data Commons Tool
    print("\n1. Testing Data Commons Tool...")
    dc_tool = DataCommonsTool()
    test_query = "What is the unemployment rate in the United States?"
    dc_result = dc_tool._run(test_query)
    print(f"   âœ“ Data Commons query: {test_query}")
    print(f"   âœ“ Method used: {dc_result.get('methodology', 'unknown')}")
    print(f"   âœ“ Data points found: {len(dc_result.get('data_points', []))}")
    if dc_result.get('data_points'):
        print(f"   âœ“ Sample result: {dc_result['data_points'][0].get('value', 'No data')[:100]}...")
    
    # Test Tavily Tool (this will make actual API calls if TAVILY_API_KEY is set)
    print("\n2. Testing Tavily Tool...")
    try:
        tavily_tool = TavilyTool()
        tavily_result = tavily_tool._run("artificial intelligence policy")
        print(f"   âœ“ Tavily search completed")
        print(f"   âœ“ Found {len(tavily_result.get('search_results', []))} results")
        if tavily_result.get('error'):
            print(f"   âš ï¸  Tavily error (expected if no API key): {tavily_result['error']}")
    except Exception as e:
        print(f"   âš ï¸  Tavily test failed (expected if no API key): {e}")
    
    # Test RAG Tool (without vectorstore)
    print("\n3. Testing RAG Tool...")
    rag_tool = RAGTool()
    rag_result = rag_tool._run("What is AI policy?")
    print(f"   âœ“ RAG tool response: {rag_result.get('answer', 'No answer')[:100]}...")
    
    print("\nâœ… Tool tests completed!")

async def test_agent():
    """Test the main agent (without actual file uploads)"""
    print("\nğŸ¤– Testing Multi-Source Analysis Agent...")
    
    agent = MultiSourceAnalysisAgent()
    
    # Test session creation
    session_id = agent.create_new_session()
    print(f"   âœ“ Created session: {session_id}")
    
    # Test session info retrieval
    session_info = agent.get_session_info(session_id)
    print(f"   âœ“ Session info retrieved: {session_info is None}")  # Should be None (no docs uploaded)
    
    # Test basic query processing
    test_query = "We need to analyze climate change policy and its impact on economic development and healthcare systems."
    print(f"   âœ“ Test query prepared: {test_query[:50]}...")
    
    # Test that the agent can handle basic operations
    print(f"   âœ“ Agent ready for document processing and queries")
    
    print("\nâœ… Agent tests completed!")

def test_config():
    """Test configuration loading"""
    print("\nâš™ï¸  Testing configuration...")
    
    from config import settings
    print(f"   âœ“ LLM Model: {settings.llm_model}")
    print(f"   âœ“ Embedding Model: {settings.embedding_model}")
    print(f"   âœ“ Chunk Size: {settings.chunk_size}")
    print(f"   âœ“ Max File Size: {settings.max_file_size // (1024*1024)}MB")
    print(f"   âœ“ Allowed File Types: {settings.allowed_file_types}")
    
    # Check API keys (without exposing them)
    print(f"   âœ“ OpenAI API Key: {'âœ… Set' if settings.openai_api_key else 'âŒ Missing'}")
    print(f"   âœ“ Tavily API Key: {'âœ… Set' if settings.tavily_api_key else 'âŒ Missing'}")
    print(f"   âœ“ LangSmith API Key: {'âœ… Set' if settings.langsmith_api_key else 'âŒ Not set (optional)'}")
    
    print("\nâœ… Configuration tests completed!")

def test_models():
    """Test Pydantic models"""
    print("\nğŸ“‹ Testing Pydantic models...")
    
    from models import StreamingUpdate, ToolType, DocumentChunk
    
    # Test StreamingUpdate
    update = StreamingUpdate(
        type="tool",
        content="Testing RAG tool",
        tool=ToolType.RAG_TOOL,
        metadata={"test": True}
    )
    print(f"   âœ“ StreamingUpdate created: {update.type}")
    
    # Test DocumentChunk
    chunk = DocumentChunk(
        content="This is a test document chunk",
        metadata={"filename": "test.pdf", "page": 1},
        page_number=1
    )
    print(f"   âœ“ DocumentChunk created: {len(chunk.content)} chars")
    
    # Test JSON serialization
    update_json = json.dumps(update.dict())
    print(f"   âœ“ JSON serialization works: {len(update_json)} chars")
    
    print("\nâœ… Model tests completed!")

async def main():
    """Run all tests"""
    print("ğŸš€ Starting Multi-Source Analysis Agent Backend Tests")
    print("=" * 60)
    
    try:
        # Test configuration first
        test_config()
        
        # Test models
        test_models()
        
        # Test tools
        await test_tools()
        
        # Test agent
        await test_agent()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ All tests completed successfully!")
        print("\nNote: Some tests may show warnings if API keys are not configured.")
        print("This is expected for development setup.")
        
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main()) 